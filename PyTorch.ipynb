{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Baseline Training\n",
    "\n",
    "In this notebook, we train a baseline model for our final project on the kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "Turns our kaggle csv of (label, pixel array, datatype) into a file dataset of greyscale images along with an index csv of filepaths + types. This format is what PyTorch data loaders will expect later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "                                               files  labels\n",
      "0  /home/dhruvamin/touchy-feely/data/pics/IMUVEQ.npy     0.0\n",
      "1  /home/dhruvamin/touchy-feely/data/pics/QH56ZP.npy     0.0\n",
      "2  /home/dhruvamin/touchy-feely/data/pics/W2ULEK.npy     2.0\n",
      "3  /home/dhruvamin/touchy-feely/data/pics/67TJ2C.npy     4.0\n",
      "4  /home/dhruvamin/touchy-feely/data/pics/DZGTQ9.npy     6.0\n",
      "val\n",
      "                                               files  labels\n",
      "0  /home/dhruvamin/touchy-feely/data/pics/KDYUQC.npy     0.0\n",
      "1  /home/dhruvamin/touchy-feely/data/pics/JP2GI0.npy     5.0\n",
      "2  /home/dhruvamin/touchy-feely/data/pics/KQHTFX.npy     6.0\n",
      "3  /home/dhruvamin/touchy-feely/data/pics/73K7WX.npy     4.0\n",
      "4  /home/dhruvamin/touchy-feely/data/pics/CNUVE5.npy     2.0\n",
      "test\n",
      "                                               files  labels\n",
      "0  /home/dhruvamin/touchy-feely/data/pics/1P8R82.npy     0.0\n",
      "1  /home/dhruvamin/touchy-feely/data/pics/TEL6T9.npy     1.0\n",
      "2  /home/dhruvamin/touchy-feely/data/pics/6H51F2.npy     4.0\n",
      "3  /home/dhruvamin/touchy-feely/data/pics/D33140.npy     6.0\n",
      "4  /home/dhruvamin/touchy-feely/data/pics/3MKETH.npy     3.0\n"
     ]
    }
   ],
   "source": [
    "#I cheated and just hard coded these from our data\n",
    "NUM_TRAIN = 28709\n",
    "NUM_VAL = 3589\n",
    "NUM_TEST = 3589\n",
    "pixel_dim = 48\n",
    "\n",
    "X_train = np.zeros((NUM_TRAIN, 1, pixel_dim, pixel_dim))\n",
    "X_val = np.zeros((NUM_VAL, 1, pixel_dim, pixel_dim))\n",
    "X_test = np.zeros((NUM_TEST, 1, pixel_dim, pixel_dim))\n",
    "y_train = np.zeros(NUM_TRAIN)\n",
    "y_val = np.zeros(NUM_VAL)\n",
    "y_test = np.zeros(NUM_TEST)\n",
    "\n",
    "\n",
    "with open('./data/fer2013.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rownum = 0\n",
    "    trainrow = 0\n",
    "    valrow = 0\n",
    "    testrow = 0\n",
    "    for row in reader:\n",
    "        if rownum != 0:\n",
    "            label = int(row[0])\n",
    "            pixels = np.fromstring(row[1], dtype=int, sep=' ')\n",
    "            usage = row[2]\n",
    "            pixarray = np.reshape(pixels,(1,pixel_dim, pixel_dim))\n",
    "            \n",
    "            if usage == 'Training':\n",
    "                X_train[trainrow, :, :, :] = pixarray\n",
    "                y_train[trainrow] = label\n",
    "                trainrow += 1\n",
    "            elif usage == 'PrivateTest':\n",
    "                X_val[valrow, :, :, :] = pixarray\n",
    "                y_val[valrow] = label\n",
    "                valrow += 1\n",
    "            else:\n",
    "                X_test[testrow, :, :, :] = pixarray\n",
    "                y_test[testrow] = label\n",
    "                testrow += 1         \n",
    "        rownum += 1\n",
    "\n",
    "# save to file\n",
    "SAVE_DIR = os.path.expanduser('~/touchy-feely/data/pics/')\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.mkdir(SAVE_DIR)\n",
    "else:\n",
    "    import shutil\n",
    "    shutil.rmtree(SAVE_DIR)\n",
    "    os.mkdir(SAVE_DIR)\n",
    "\n",
    "loop = 0\n",
    "names = ['train', 'val', 'test']\n",
    "for (X,Y) in [(X_train, y_train), (X_val, y_val), (X_test, y_test)]: \n",
    "    print(names[loop])\n",
    "    paths = []\n",
    "    for x in X:\n",
    "\n",
    "        file_path = os.path.join(SAVE_DIR,''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6))\n",
    "    )\n",
    "        #print(file_path+'.npy')\n",
    "        np.save('%s.npy' % file_path, x)\n",
    "        paths.append(file_path+'.npy')\n",
    "\n",
    "    # create data frame from file paths and labels\n",
    "    df = pd.DataFrame(data={'files':paths, 'labels':Y})\n",
    "    print(df.head())\n",
    "\n",
    "    # save data frame as CSV file\n",
    "    df.to_csv(os.path.join(SAVE_DIR, names[loop] + '_DATA.csv'), index=False)\n",
    "    loop += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We load the kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train).int()\n",
    "y_val_tensor = torch.from_numpy(y_val).int()\n",
    "y_test_tensor = torch.from_numpy(y_test).int()\n",
    "\n",
    "greyscale_train = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "loader_train = DataLoader(greyscale_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN,0))\n",
    "\n",
    "greyscale_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "loader_val = DataLoader(greyscale_val, batch_size=64, sampler=ChunkSampler(NUM_VAL,0))\n",
    "\n",
    "greyscale_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "loader_test = DataLoader(greyscale_test, batch_size=64, sampler=ChunkSampler(NUM_TEST,0))\n",
    "    \n",
    "#NUM_TRAIN = 49000\n",
    "#NUM_VAL = 1000\n",
    "\n",
    "#cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,transform=T.ToTensor())\n",
    "#loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "#cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,transform=T.ToTensor())\n",
    "#loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "#cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,transform=T.ToTensor())\n",
    "#loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dtype = torch.FloatTensor # the CPU datatype\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Flatten Function\n",
    "\n",
    "Remember that our image data (and more relevantly, our intermediate feature maps) are initially N x C x H x W, where:\n",
    "* N is the number of datapoints\n",
    "* C is the number of channels\n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the height of the intermediate feature map in pixels\n",
    "\n",
    "The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Check\n",
    "\n",
    "If this returns false, or otherwise fails in a not-graceful way (i.e., with some error message), you may not have an NVIDIA GPU available on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    \n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    #if loader.dataset.train:\n",
    "    #    print('Checking accuracy on validation set')\n",
    "    #else:\n",
    "    #    print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "\n",
    "model = None\n",
    "loss_fn = None\n",
    "optimizer = None\n",
    "\n",
    "num_filters1 = 32\n",
    "num_filters2 = 96\n",
    "num_filters3 = 256\n",
    "num_filters4 = 512\n",
    "kernal_size1 = 7\n",
    "kernal_size2 = 5\n",
    "kernal_size3 =3\n",
    "affine_layer_size1 = 4048\n",
    "affine_layer_size2 = 1024\n",
    "num_epochs = 5\n",
    "\n",
    "# This one is currently overwritten, but thought it might be useful later\n",
    "model_base = nn.Sequential( # You fill this in!\n",
    "                nn.Conv2d(1, num_filters1, kernel_size=kernal_size1, stride=1), #Hout = (input_size - kernal_size)/stride + 1\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(num_filters1),\n",
    "                nn.Conv2d(num_filters1, num_filters2, kernel_size=kernal_size2, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2, stride=2), #Hout = (input_size - pool_size)/stride + 1\n",
    "                nn.Conv2d(num_filters2, num_filters3, kernel_size=kernal_size3, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2, stride=2), #Hout = (input_size - pool_size)/stride + 1\n",
    "                nn.Conv2d(num_filters3, num_filters4, kernel_size=kernal_size3, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2, stride=2), #Hout = (input_size - pool_size)/stride + 1\n",
    "                Flatten(),                   \n",
    "                nn.Linear(3*3*num_filters4, affine_layer_size1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(affine_layer_size1, affine_layer_size2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(affine_layer_size2,7),\n",
    "            )\n",
    "\n",
    "\n",
    "model = model_base.type(gpu_dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # lr sets the learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 5\n",
      "t = 100, loss = 1.8142\n",
      "t = 200, loss = 1.7888\n",
      "t = 300, loss = 1.5804\n",
      "t = 400, loss = 1.7315\n",
      "Starting epoch 2 / 5\n",
      "t = 100, loss = 1.5529\n",
      "t = 200, loss = 1.6496\n",
      "t = 300, loss = 1.4338\n",
      "t = 400, loss = 1.6402\n",
      "Starting epoch 3 / 5\n",
      "t = 100, loss = 1.4893\n",
      "t = 200, loss = 1.4649\n",
      "t = 300, loss = 1.3010\n",
      "t = 400, loss = 1.6161\n",
      "Starting epoch 4 / 5\n",
      "t = 100, loss = 1.3687\n",
      "t = 200, loss = 1.4486\n",
      "t = 300, loss = 1.2450\n",
      "t = 400, loss = 1.5707\n",
      "Starting epoch 5 / 5\n",
      "t = 100, loss = 1.3184\n",
      "t = 200, loss = 1.4567\n",
      "t = 300, loss = 1.2543\n",
      "t = 400, loss = 1.5743\n",
      "Got 14086 / 28709 correct (49.06)\n",
      "Got 1637 / 3589 correct (45.61)\n"
     ]
    }
   ],
   "source": [
    "model.apply(reset)\n",
    "train(model, loss_fn, optimizer, num_epochs=num_epochs)\n",
    "check_accuracy(model, loader_train)\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model).  This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "check_accuracy(best_model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). One note: what we call in the class \"spatial batch norm\" is called \"BatchNorm2D\" in PyTorch.\n",
    "\n",
    "* Layers: http://pytorch.org/docs/nn.html\n",
    "* Activations: http://pytorch.org/docs/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further with PyTorch\n",
    "\n",
    "The next assignment will make heavy use of PyTorch. You might also find it useful for your projects. \n",
    "\n",
    "Here's a nice tutorial by Justin Johnson that shows off some of PyTorch's features, like dynamic graphs and custom NN modules: http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "If you're interested in reinforcement learning for your final project, this is a good (more advanced) DQN tutorial in PyTorch: http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
